{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequence to Sequence and Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Character Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nlpia.loaders import get_data\n",
    "df = get_data('moviedialog')\n",
    "input_texts, target_texts = [], []\n",
    "# holds seen files in the input and target text\n",
    "# we use a set to get UNIQUE characters to build one-hot matrices\n",
    "input_vocabulary = set()\n",
    "output_vocabulary = set()\n",
    "start_token = '\\t'\n",
    "stop_token = '\\n'\n",
    "#max_training_samples: num lines used for training\n",
    "max_training_samples = min(25000, len(df)-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>statement</th>\n",
       "      <th>reply</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>you're asking me out. that's so cute. what's y...</td>\n",
       "      <td>forget it.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>no, no, it's my fault we didn't have a proper ...</td>\n",
       "      <td>cameron.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>gosh, if only we could find kat a boyfriend...</td>\n",
       "      <td>let me see what i can do.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                           statement  \\\n",
       "0  you're asking me out. that's so cute. what's y...   \n",
       "1  no, no, it's my fault we didn't have a proper ...   \n",
       "2     gosh, if only we could find kat a boyfriend...   \n",
       "\n",
       "                       reply  \n",
       "0                 forget it.  \n",
       "1                   cameron.  \n",
       "2  let me see what i can do.  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "for input_text, target_text in zip(df.statement, df.reply):\n",
    "    target_text = start_token + target_text + stop_token\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_vocabulary:\n",
    "            input_vocabulary.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in output_vocabulary: \n",
    "            output_vocabulary.add(char)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Character Model Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_vocabulary = sorted(input_vocabulary)\n",
    "output_vocabulary = sorted(output_vocabulary)\n",
    "\n",
    "input_vocab_size = len(input_vocabulary)\n",
    "output_vocab_size = len(output_vocabulary)\n",
    "\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_vocabulary)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(output_vocabulary)])\n",
    "\n",
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Character Encoder-Decoder Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "#shape: num_samples, max_len_sequence, num_unique_tokens_in_vocab\n",
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_length, input_vocab_size), dtype='float32')\n",
    "decoder_input_data = np.zeros((len(input_texts), max_decoder_seq_length, output_vocab_size), dtype='float32')\n",
    "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_length, output_vocab_size), dtype='float32')\n",
    "\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        #decoder target data is one time step behind decoder input data\n",
    "        if t>0:\n",
    "            decoder_target_data[i, t-1, target_token_index[char]] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct and Train Character Sequence Encoder-Decoder Network\n",
    "We have converted the preprocessed corpus into input and target samples, created index lookup dictionaries, and converted the samples into one-hot tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-01-07 17:05:33,462 WARNING:     tensorflow:506:            new_func From /Users/tomjoshi/anaconda3/envs/nlpiaenv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "2021-01-07 17:05:33,909 WARNING:     tensorflow:323:            new_func From /Users/tomjoshi/anaconda3/envs/nlpiaenv/lib/python3.7/site-packages/tensorflow_core/python/ops/math_grad.py:1424: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "2021-01-07 17:05:34,779 WARNING:     tensorflow:139:_tfmw_add_deprecation_warning From /Users/tomjoshi/anaconda3/envs/nlpiaenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 57915 samples, validate on 6436 samples\n",
      "Epoch 1/100\n",
      "57915/57915 [==============================] - 6905s 119ms/step - loss: 0.7673 - acc: 0.1185 - val_loss: 0.6519 - val_acc: 0.1537\n",
      "Epoch 2/100\n",
      "57915/57915 [==============================] - 304s 5ms/step - loss: 0.5950 - acc: 0.1608 - val_loss: 0.5741 - val_acc: 0.1742\n",
      "Epoch 3/100\n",
      "57915/57915 [==============================] - 305s 5ms/step - loss: 0.5408 - acc: 0.1756 - val_loss: 0.5405 - val_acc: 0.1831\n",
      "Epoch 4/100\n",
      "57915/57915 [==============================] - 309s 5ms/step - loss: 0.5122 - acc: 0.1831 - val_loss: 0.5205 - val_acc: 0.1886\n",
      "Epoch 5/100\n",
      "57915/57915 [==============================] - 309s 5ms/step - loss: 0.4943 - acc: 0.1880 - val_loss: 0.5059 - val_acc: 0.1928\n",
      "Epoch 6/100\n",
      "57915/57915 [==============================] - 311s 5ms/step - loss: 0.4816 - acc: 0.1915 - val_loss: 0.4971 - val_acc: 0.1949\n",
      "Epoch 7/100\n",
      "57915/57915 [==============================] - 326s 6ms/step - loss: 0.4721 - acc: 0.1941 - val_loss: 0.4908 - val_acc: 0.1967\n",
      "Epoch 8/100\n",
      "57915/57915 [==============================] - 310s 5ms/step - loss: 0.4647 - acc: 0.1961 - val_loss: 0.4853 - val_acc: 0.1984\n",
      "Epoch 9/100\n",
      "57915/57915 [==============================] - 295s 5ms/step - loss: 0.4584 - acc: 0.1978 - val_loss: 0.4821 - val_acc: 0.1993\n",
      "Epoch 10/100\n",
      "57915/57915 [==============================] - 295s 5ms/step - loss: 0.4532 - acc: 0.1993 - val_loss: 0.4790 - val_acc: 0.1999\n",
      "Epoch 11/100\n",
      "57915/57915 [==============================] - 324s 6ms/step - loss: 0.4486 - acc: 0.2004 - val_loss: 0.4766 - val_acc: 0.2010\n",
      "Epoch 12/100\n",
      "57915/57915 [==============================] - 310s 5ms/step - loss: 0.4446 - acc: 0.2015 - val_loss: 0.4750 - val_acc: 0.2009\n",
      "Epoch 13/100\n",
      "57915/57915 [==============================] - 309s 5ms/step - loss: 0.4411 - acc: 0.2024 - val_loss: 0.4729 - val_acc: 0.2019\n",
      "Epoch 14/100\n",
      "57915/57915 [==============================] - 316s 5ms/step - loss: 0.4379 - acc: 0.2033 - val_loss: 0.4716 - val_acc: 0.2018\n",
      "Epoch 15/100\n",
      "57915/57915 [==============================] - 314s 5ms/step - loss: 0.4349 - acc: 0.2041 - val_loss: 0.4714 - val_acc: 0.2023\n",
      "Epoch 16/100\n",
      "57915/57915 [==============================] - 299s 5ms/step - loss: 0.4323 - acc: 0.2048 - val_loss: 0.4708 - val_acc: 0.2025\n",
      "Epoch 17/100\n",
      "57915/57915 [==============================] - 303s 5ms/step - loss: 0.4299 - acc: 0.2055 - val_loss: 0.4699 - val_acc: 0.2028\n",
      "Epoch 18/100\n",
      "57915/57915 [==============================] - 313s 5ms/step - loss: 0.4276 - acc: 0.2060 - val_loss: 0.4709 - val_acc: 0.2027\n",
      "Epoch 19/100\n",
      "57915/57915 [==============================] - 321s 6ms/step - loss: 0.4255 - acc: 0.2065 - val_loss: 0.4697 - val_acc: 0.2035\n",
      "Epoch 20/100\n",
      "57915/57915 [==============================] - 304s 5ms/step - loss: 0.4235 - acc: 0.2072 - val_loss: 0.4698 - val_acc: 0.2033\n",
      "Epoch 21/100\n",
      "57915/57915 [==============================] - 2820s 49ms/step - loss: 0.4218 - acc: 0.2076 - val_loss: 0.4698 - val_acc: 0.2032\n",
      "Epoch 22/100\n",
      "36352/57915 [=================>............] - ETA: 1:54 - loss: 0.4189 - acc: 0.2087"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-943b15c605be>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m     validation_split=0.1)\n\u001b[0m\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlpiaenv/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1239\u001b[0;31m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[1;32m   1240\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[0;32m~/anaconda3/envs/nlpiaenv/lib/python3.7/site-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[1;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlpiaenv/lib/python3.7/site-packages/tensorflow_core/python/keras/backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   3474\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3475\u001b[0m     fetched = self._callable_fn(*array_vals,\n\u001b[0;32m-> 3476\u001b[0;31m                                 run_metadata=self.run_metadata)\n\u001b[0m\u001b[1;32m   3477\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_fetch_callbacks\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3478\u001b[0m     output_structure = nest.pack_sequence_as(\n",
      "\u001b[0;32m~/anaconda3/envs/nlpiaenv/lib/python3.7/site-packages/tensorflow_core/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1470\u001b[0m         ret = tf_session.TF_SessionRunCallable(self._session._session,\n\u001b[1;32m   1471\u001b[0m                                                \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1472\u001b[0;31m                                                run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1473\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1474\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense\n",
    "\n",
    "batch_size = 64\n",
    "epochs = 100\n",
    "num_neurons = 256\n",
    "\n",
    "encoder_inputs = Input(shape=(None, input_vocab_size))\n",
    "encoder = LSTM(num_neurons, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "\n",
    "decoder_inputs = Input(shape=(None, output_vocab_size))\n",
    "decoder_lstm = LSTM(num_neurons, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(output_vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy', metrics=['acc'])\n",
    "model.fit(\n",
    "    [encoder_input_data, decoder_input_data], \n",
    "    decoder_target_data, \n",
    "    batch_size=batch_size, \n",
    "    epochs=epochs,\n",
    "    validation_split=0.1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construct Response Generator Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "thought_input = [Input(shape=(num_neurons,)), Input(shape=(num_neurons,))]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=thought_input)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model(\n",
    "    inputs=[decoder_inputs] + thought_input,\n",
    "    output=[decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([decoder_inputs] + thought_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print([decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build a character-based translator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Input: the one hot encoding of a character\n",
    "Output: The generated series of characters\n",
    "'''\n",
    "def decode_sequence(input_seq):\n",
    "    #The thought vector will be the input to the decoder\n",
    "    thought = encoder_model.predict(input_seq)\n",
    "    \n",
    "    target_seq = np.zeros((1, 1, output_vocab_size))\n",
    "    target_seq[0, 0, target_token_index[stop_token]] = 1.\n",
    "    \n",
    "    stop_condition = False\n",
    "    generated_sequence = ''\n",
    "    \n",
    "    while not stop_condition:\n",
    "        # Passing the already-generated tokens and the tatest state to the decoder to predict next sequence\n",
    "        output_tokens, h, c = decoder.model.predict([target_seq]+thought)\n",
    "        \n",
    "        generated_token_idx = np.argmax(output_tokens[0, -1, :])\n",
    "        generated_char = reverse_target_char_index[generated_token_idx]\n",
    "        generated_sequence += generated_char\n",
    "        if (generated_char == stop_token or len(generated_sequence) > max_decoder_seq_length):\n",
    "            stop_condition = True\n",
    "            # Update the target sequence and use the last generated token as the input to the next generation step\n",
    "            target_seq = np.zeros((1, 1, output_vocab_size))\n",
    "            target_seq[0, 0, generated_token_idx] = 1.\n",
    "            thought = [h, c]\n",
    "        return generated_sequence\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(input_text):\n",
    "    input_seq = np.zeros((1, max_encoder_seq_length, input_vocab_size), dtype='float32')\n",
    "    for t, char in enumerate(input_text):\n",
    "        input_seq[0, t, input_token_index[char]] = 1.\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(\"Bot Reply (Decoded sentence): \", decoded_sentence)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
