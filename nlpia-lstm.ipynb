{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from random import shuffle\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(filepath):\n",
    "    positive_path = os.path.join(filepath, \"pos\")\n",
    "    negative_path = os.path.join(filepath, \"neg\")\n",
    "    pos_label = 1\n",
    "    neg_label = 0\n",
    "    dataset = []\n",
    "    for filename in glob.glob(os.path.join(positive_path, \"*.txt\")):\n",
    "        with open(filename, \"r\") as f:\n",
    "            dataset.append((pos_label, f.read()))\n",
    "    for filename in glob.glob(os.path.join(negative_path, \"*.txt\")):\n",
    "        with open(filename, \"r\") as f:\n",
    "            dataset.append((neg_label, f.read()))\n",
    "    shuffle(dataset)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer and Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_vectorize(dataset):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    vectorized_data = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenizer.tokenize(sample[1])\n",
    "        sample_vecs = []\n",
    "        for token in tokens:\n",
    "            try: \n",
    "                sample_vecs.append(word_vectors[token])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        vectorized_data.append(sample_vecs)\n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Expected Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_expected(dataset):\n",
    "    expected = []\n",
    "    for sample in dataset:\n",
    "        expected.append(sample[0])\n",
    "    return expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'preprocess_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-932f24ce7315>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"../Datasets/aclimdb/train\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mvectorized_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenize_and_vectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mexpected\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcollect_expected\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0msplit_point\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorized_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mx_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorized_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0msplit_point\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'preprocess_data' is not defined"
     ]
    }
   ],
   "source": [
    "dataset = preprocess_data(\"../Datasets/aclimdb/train\")\n",
    "vectorized_data = tokenize_and_vectorize(dataset)\n",
    "expected = collect_expected(dataset)\n",
    "split_point = int(len(vectorized_data)*0.8)\n",
    "x_train = vectorized_data[:split_point]\n",
    "y_train = expected[:split_point]\n",
    "x_test = vectorized_data[split_point:]\n",
    "y_test = expected[split_point:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 300\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Padding and Truncating Token Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_trunc(data, maxlen):\n",
    "    '''\n",
    "    For a given dataset, pad with zero vectors or truncate to maxlen\n",
    "    '''\n",
    "    new_data = []\n",
    "    \n",
    "    # vector of 0 the length of the word vectors\n",
    "    zero_vector = []\n",
    "    for _ in range(len(data[0][0])):\n",
    "        zero_vector.append(0.0)\n",
    "    \n",
    "    #Iterate through rows, truncate if too big, add zero vectors if too small\n",
    "    for sample in data:\n",
    "        temp = []\n",
    "        if len(sample) > maxlen:\n",
    "            temp = sample[:maxlen]\n",
    "        elif len(sample) < maxlen:\n",
    "            temp = sample\n",
    "            additional_elems = maxlen-len(sample)\n",
    "            for _ in range(additional_elems):\n",
    "                temp.append(zero_vector)\n",
    "        else:\n",
    "            temp = sample\n",
    "        new_data.append(temp)\n",
    "    return new_data\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = pad_trunc(x_train, maxlen)\n",
    "x_test = pad_trunc(x_test, maxlen)\n",
    "\n",
    "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tomjoshi/anaconda3/envs/nlpiaenv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "WARNING:tensorflow:From /Users/tomjoshi/anaconda3/envs/nlpiaenv/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 400, 50)           70200     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 400, 50)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 20000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 20001     \n",
      "=================================================================\n",
      "Total params: 90,201\n",
      "Trainable params: 90,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM\n",
    "\n",
    "num_neurons = 50\n",
    "model = Sequential()\n",
    "model.add(LSTM(num_neurons,\n",
    "               return_sequences=True,\n",
    "               input_shape=(maxlen, embedding_dims)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile('rmsprop', 'binary_crossentropy', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tomjoshi/anaconda3/envs/nlpiaenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 185s 9ms/step - loss: 0.4693 - accuracy: 0.7789 - val_loss: 0.3569 - val_accuracy: 0.8518\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 204s 10ms/step - loss: 0.3484 - accuracy: 0.8543 - val_loss: 0.3411 - val_accuracy: 0.8550\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fc67bd28e90>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_structure = model.to_json()\n",
    "with open(\"models_and_json/lstm_model1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "model.save_weights(\"models_and_json/lstm_weights1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "with open(\"models_and_json/lstm_model1.json\", \"r\") as json_file:\n",
    "    json_model = json_file.read()\n",
    "model = model_from_json(json_model)\n",
    "model.load_weights(\"models_and_json/lstm_weights1.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1 = \"\"\"I hate that the dismal weather had me down for so long,\n",
    "... when will it break! Ugh, when does happiness return? The sun is\n",
    "... blinding and the puffy clouds are too thin. I can't wait for the\n",
    "... weekend.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample's sentiment, 1 - pos, 2 - neg : [[0]]\n"
     ]
    }
   ],
   "source": [
    "vec_list = tokenize_and_vectorize([(1,sample_1)])\n",
    "test_vec_list = pad_trunc(vec_list, maxlen)\n",
    "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
    "print(\"Sample's sentiment, 1 - pos, 2 - neg : {}\".format(model.predict_classes(test_vec)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw output of sigmoid function: [[0.2482055]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Raw output of sigmoid function: {}\".format(model.predict(test_vec)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Data for Embedding Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_len(data, maxlen):\n",
    "    total_len = truncated = padded = exact = 0 \n",
    "    for sample in data:\n",
    "        total_len+=len(sample)\n",
    "        if len(sample) > maxlen: \n",
    "            truncated += 1\n",
    "        elif len(sample) < maxlen: \n",
    "            padded += 1\n",
    "        else: \n",
    "            exact += 1\n",
    "    print(\"Padded: {}\".format(padded))\n",
    "    print(\"Equal: {}\".format(exact))\n",
    "    print(\"Truncated: {}\".format(truncated))\n",
    "    print(\"Avg length: {}\".format(total_len/len(data)))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padded: 22458\n",
      "Equal: 20\n",
      "Truncated: 2522\n",
      "Avg length: 205.2144\n"
     ]
    }
   ],
   "source": [
    "dataset = preprocess_data('../Datasets/aclimdb/train')\n",
    "vectorized_data = tokenize_and_vectorize(dataset)\n",
    "test_len(vectorized_data, 400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimize LSTM Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, LSTM\n",
    "\n",
    "maxlen = 200\n",
    "batch_size = 32\n",
    "embedding_dims = 300\n",
    "epochs = 2\n",
    "num_neurons = 50\n",
    "\n",
    "dataset = preprocess_data('../Datasets/aclimdb/train')\n",
    "vectorized_data = tokenize_and_vectorize(dataset)\n",
    "expected = collect_expected(dataset)\n",
    "split_point = int(len(vectorized_data)*0.8)\n",
    "\n",
    "x_train = vectorized_data[:split_point]\n",
    "x_train = pad_trunc(x_train, maxlen)\n",
    "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
    "\n",
    "y_train = expected[:split_point]\n",
    "y_train = np.array(y_train)\n",
    "\n",
    "x_test = vectorized_data[split_point:]\n",
    "x_test = pad_trunc(x_test, maxlen)\n",
    "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
    "\n",
    "y_test = expected[split_point:]\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_2 (LSTM)                (None, 200, 50)           70200     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 200, 50)           0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 10000)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 10001     \n",
      "=================================================================\n",
      "Total params: 80,201\n",
      "Trainable params: 80,201\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(LSTM(num_neurons, return_sequences=True,\n",
    "               input_shape=(maxlen, embedding_dims)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation='sigmoid'))\n",
    "model.compile('rmsprop', 'binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tomjoshi/anaconda3/envs/nlpiaenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 91s 5ms/step - loss: 0.4759 - accuracy: 0.7749 - val_loss: 0.3867 - val_accuracy: 0.8350\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 64s 3ms/step - loss: 0.3622 - accuracy: 0.8429 - val_loss: 0.3498 - val_accuracy: 0.8560\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7ffa81c19f90>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_structure = model.to_json()\n",
    "with open(\"models_and_json/lstm_model2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "model.save_weights(\"models_and_json/lstm_weights2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shakespeare Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package gutenberg to\n",
      "[nltk_data]     /Users/tomjoshi/nltk_data...\n",
      "[nltk_data]   Unzipping corpora/gutenberg.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('gutenberg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['austen-emma.txt',\n",
       " 'austen-persuasion.txt',\n",
       " 'austen-sense.txt',\n",
       " 'bible-kjv.txt',\n",
       " 'blake-poems.txt',\n",
       " 'bryant-stories.txt',\n",
       " 'burgess-busterbrown.txt',\n",
       " 'carroll-alice.txt',\n",
       " 'chesterton-ball.txt',\n",
       " 'chesterton-brown.txt',\n",
       " 'chesterton-thursday.txt',\n",
       " 'edgeworth-parents.txt',\n",
       " 'melville-moby_dick.txt',\n",
       " 'milton-paradise.txt',\n",
       " 'shakespeare-caesar.txt',\n",
       " 'shakespeare-hamlet.txt',\n",
       " 'shakespeare-macbeth.txt',\n",
       " 'whitman-leaves.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import gutenberg\n",
    "gutenberg.fileids()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preproces Shakespeare Plays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'corpus length:375542 total chars:50'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = ''\n",
    "for txt in gutenberg.fileids():\n",
    "    if 'shakespeare' in txt:\n",
    "        text += gutenberg.raw(txt).lower()\n",
    "chars = sorted(list(set(text)))\n",
    "char_indices = dict((c, i) for i, c in enumerate(chars))\n",
    "indices_char = dict((i, c) for i, c in enumerate(chars))\n",
    "'corpus length:{} total chars:{}'.format(len(text), len(chars)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[the tragedie of julius caesar by william shakespeare 1599]\n",
      "\n",
      "\n",
      "actus primus. scoena prima.\n",
      "\n",
      "enter flauius, murellus, and certaine commoners ouer the stage.\n",
      "\n",
      "  flauius. hence: home you idle creatures, get you home:\n",
      "is this a holiday? what, know you not\n",
      "(being mechanicall) you ought not walke\n",
      "vpon a labouring day, without the signe\n",
      "of your profession? speake, what trade art thou?\n",
      "  car. why sir, a carpenter\n",
      "\n",
      "   mur. where is thy leather apron, and thy rule?\n",
      "what dost thou with thy best apparrell on\n"
     ]
    }
   ],
   "source": [
    "print(text[:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Assemble Training Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nb sequences:  125168\n"
     ]
    }
   ],
   "source": [
    "maxlen = 40\n",
    "step = 3\n",
    "sentences = []\n",
    "next_chars = []\n",
    "for i in range(0, len(text) - maxlen, step):\n",
    "    sentences.append(text[i: i+maxlen])\n",
    "    next_chars.append(text[i+maxlen])\n",
    "# get number of training samples\n",
    "print('nb sequences: ', len(sentences))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## One Hot Encode Training Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "X = np.zeros((len(sentences), maxlen, len(chars)), dtype=np.bool)\n",
    "y = np.zeros((len(sentences), len(chars)), dtype=np.bool)\n",
    "for i, sentence in enumerate(sentences):\n",
    "    for t, char in enumerate(sentence):\n",
    "        X[i, t, char_indices[char]] = 1\n",
    "    y[i, char_indices[next_chars[i]]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tomjoshi/anaconda3/envs/nlpiaenv/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "lstm_1 (LSTM)                (None, 128)               91648     \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 50)                6450      \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 50)                0         \n",
      "=================================================================\n",
      "Total params: 98,098\n",
      "Trainable params: 98,098\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation\n",
    "from keras.layers import LSTM\n",
    "from keras.optimizers import RMSprop\n",
    "model = Sequential()\n",
    "# input_shape = (features, time steps)\n",
    "model.add(LSTM(128,\n",
    "               input_shape=(maxlen, len(chars))))\n",
    "model.add(Dense(len(chars)))\n",
    "model.add(Activation('softmax'))\n",
    "optimizer = RMSprop(lr=0.01)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=optimizer)\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tomjoshi/anaconda3/envs/nlpiaenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Epoch 1/6\n",
      "125168/125168 [==============================] - 49s 394us/step - loss: 2.0697\n",
      "Epoch 2/6\n",
      "125168/125168 [==============================] - 48s 384us/step - loss: 1.6987\n",
      "Epoch 3/6\n",
      "125168/125168 [==============================] - 48s 387us/step - loss: 1.5907\n",
      "Epoch 4/6\n",
      "125168/125168 [==============================] - 48s 382us/step - loss: 1.5248\n",
      "Epoch 5/6\n",
      "125168/125168 [==============================] - 48s 383us/step - loss: 1.4814\n",
      "Epoch 6/6\n",
      "125168/125168 [==============================] - 48s 381us/step - loss: 1.4480\n",
      "Epoch 1/6\n",
      "125168/125168 [==============================] - 48s 385us/step - loss: 1.4220\n",
      "Epoch 2/6\n",
      "125168/125168 [==============================] - 48s 382us/step - loss: 1.3979\n",
      "Epoch 3/6\n",
      "125168/125168 [==============================] - 48s 382us/step - loss: 1.3824\n",
      "Epoch 4/6\n",
      "125168/125168 [==============================] - 48s 383us/step - loss: 1.3683\n",
      "Epoch 5/6\n",
      "125168/125168 [==============================] - 48s 382us/step - loss: 1.3554\n",
      "Epoch 6/6\n",
      "125168/125168 [==============================] - 48s 382us/step - loss: 1.3440\n",
      "Epoch 1/6\n",
      "125168/125168 [==============================] - 48s 381us/step - loss: 1.3316\n",
      "Epoch 2/6\n",
      "125168/125168 [==============================] - 48s 384us/step - loss: 1.3223\n",
      "Epoch 3/6\n",
      "125168/125168 [==============================] - 690s 6ms/step - loss: 1.3150\n",
      "Epoch 4/6\n",
      "125168/125168 [==============================] - 63s 500us/step - loss: 1.3078\n",
      "Epoch 5/6\n",
      "125168/125168 [==============================] - 50s 401us/step - loss: 1.3017\n",
      "Epoch 6/6\n",
      "125168/125168 [==============================] - 50s 396us/step - loss: 1.2954\n",
      "Epoch 1/6\n",
      "125168/125168 [==============================] - 49s 391us/step - loss: 1.2887\n",
      "Epoch 2/6\n",
      "125168/125168 [==============================] - 47s 378us/step - loss: 1.2820\n",
      "Epoch 3/6\n",
      "125168/125168 [==============================] - 51s 404us/step - loss: 1.2785\n",
      "Epoch 4/6\n",
      "125168/125168 [==============================] - 54s 428us/step - loss: 1.2719\n",
      "Epoch 5/6\n",
      "125168/125168 [==============================] - 49s 395us/step - loss: 1.2674\n",
      "Epoch 6/6\n",
      "125168/125168 [==============================] - 48s 385us/step - loss: 1.2638\n",
      "Epoch 1/6\n",
      "125168/125168 [==============================] - 48s 381us/step - loss: 1.2585\n",
      "Epoch 2/6\n",
      "125168/125168 [==============================] - 48s 383us/step - loss: 1.2560\n",
      "Epoch 3/6\n",
      "125168/125168 [==============================] - 48s 384us/step - loss: 1.2513\n",
      "Epoch 4/6\n",
      "125168/125168 [==============================] - 48s 384us/step - loss: 1.2484\n",
      "Epoch 5/6\n",
      "125168/125168 [==============================] - 50s 402us/step - loss: 1.2425\n",
      "Epoch 6/6\n",
      "125168/125168 [==============================] - 50s 396us/step - loss: 1.2424\n"
     ]
    }
   ],
   "source": [
    "epochs = 6\n",
    "batch_size = 128\n",
    "model_structure = model.to_json()\n",
    "with open(\"models_and_json/shakespeare_lstm_model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "\n",
    "for i in range(5):\n",
    "    model.fit(X, y, batch_size=batch_size, epochs=epochs)\n",
    "    model.save_weights(\"models_and_json/{}_shakespeare_lstm_weights.h5\".format(i+1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def sample(preds, temperature=1.0):\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "    return np.argmax(probas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Text With Diversity Levels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Random piece of 40 characters from original text and predicitng what character will come next.\n",
    "2. Append predicted charater to the input sentence, drop the first character, and predict again on those 40 characters as input.\n",
    "3. Flush so that characters immediatley goes to console"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "----- diversity:  0.2\n",
      "----- Generating with seed: \"aue't.\n",
      "oh good horatio, what a wounded n\"\n",
      "aue't.\n",
      "oh good horatio, what a wounded no more of his lady\n",
      "\n",
      "   cassi. thou know the strangers that stration, and that haue beares by his lady\n",
      "\n",
      "   cassi. thou know the sounder of the strangers beare\n",
      "the commis'd to my selfe of the words of the sould,\n",
      "that i shall be so strong the season on the commistion\n",
      "by the strangers blood,\n",
      "enter king with the soule, and make once,\n",
      "and with a such a shall be a brutus, and a man,\n",
      "then they shall haue \n",
      "\n",
      "----- diversity:  0.5\n",
      "----- Generating with seed: \"aue't.\n",
      "oh good horatio, what a wounded n\"\n",
      "aue't.\n",
      "oh good horatio, what a wounded no more more and to the more out,\n",
      "with more pittion browes many like a deed a tent\n",
      "do strong this burres my lord\n",
      "\n",
      "   ham. thou hast day with the stands?\n",
      "  ham. what sweare my lord\n",
      "\n",
      "   ham. in the true: and his owne commandmoke and death,\n",
      "and more happy to the drinke byds\n",
      "murther of this caesar shall he words in fortune,\n",
      "and i will blacke with sorride and not and thriffe,\n",
      "that we can you are against\n",
      "\n",
      "----- diversity:  1.0\n",
      "----- Generating with seed: \"aue't.\n",
      "oh good horatio, what a wounded n\"\n",
      "aue't.\n",
      "oh good horatio, what a wounded now, his inceype\n",
      "out that caesar caesar haue vnuinet did my blowes\n",
      "it ranke lepions to thy stible\n",
      "\n",
      "   ham. bade andntis father'lls his mabs, at vaynies\n",
      "\n",
      "   ham. why gece,\n",
      "as three faged, i my mighty to you; hhe doand with my sinnew igine,\n",
      "out the lepscetne faire to itaty; tell minyes,\n",
      "that hamlet i must tame this appera'd greefe\n",
      "to earnys of wines, that what lay a haue day,\n",
      "with and selfe more wise\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "start_index = random.randint(0, len(text) - maxlen - 1)\n",
    "for diversity in [0.2, 0.5, 1.0]:\n",
    "    print()\n",
    "    print(\"----- diversity: \", diversity)\n",
    "    generated = ''\n",
    "    sentence = text[start_index: start_index+maxlen]\n",
    "    generated += sentence\n",
    "    print('----- Generating with seed: \"' + sentence + '\"')\n",
    "    sys.stdout.write(generated)\n",
    "    for i in range(400):\n",
    "        x = np.zeros((1, maxlen, len(chars)))\n",
    "        for t, char in enumerate(sentence):\n",
    "            x[0, t, char_indices[char]] = 1.\n",
    "        preds = model.predict(x, verbose=0)[0]\n",
    "        next_index = sample(preds, diversity)\n",
    "        next_char = indices_char[next_index]\n",
    "        generated += next_char\n",
    "        # Drop first character to maintain same length\n",
    "        sentence = sentence[1:] + next_char\n",
    "        sys.stdout.write(next_char)\n",
    "        sys.stdout.flush()\n",
    "    print()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
