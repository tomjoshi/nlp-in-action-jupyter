{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from random import shuffle\n",
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin.gz', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(filepath):\n",
    "    positive_path = os.path.join(filepath, \"pos\")\n",
    "    negative_path = os.path.join(filepath, \"neg\")\n",
    "    pos_label = 1\n",
    "    neg_label = 0\n",
    "    dataset = []\n",
    "    for filename in glob.glob(os.path.join(positive_path, \"*.txt\")):\n",
    "        with open(filename, \"r\") as f:\n",
    "            dataset.append((pos_label, f.read()))\n",
    "    for filename in glob.glob(os.path.join(negative_path, \"*.txt\")):\n",
    "        with open(filename, \"r\") as f:\n",
    "            dataset.append((neg_label, f.read()))\n",
    "    shuffle(dataset)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer and Vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_vectorize(dataset):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    vectorized_data = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenizer.tokenize(sample[1])\n",
    "        sample_vecs = []\n",
    "        for token in tokens:\n",
    "            try: \n",
    "                sample_vecs.append(word_vectors[token])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        vectorized_data.append(sample_vecs)\n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pull Expected Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_expected(dataset):\n",
    "    expected = []\n",
    "    for sample in dataset:\n",
    "        expected.append(sample[0])\n",
    "    return expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = preprocess_data(\"../Datasets/aclimdb/train\")\n",
    "vectorized_data = tokenize_and_vectorize(dataset)\n",
    "expected = collect_expected(dataset)\n",
    "split_point = int(len(vectorized_data)*0.8)\n",
    "x_train = vectorized_data[:split_point]\n",
    "y_train = expected[:split_point]\n",
    "x_test = vectorized_data[split_point:]\n",
    "y_test = expected[split_point:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Network Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 400\n",
    "batch_size = 32\n",
    "embedding_dims = 300\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Padding and Truncating Token Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_trunc(data, maxlen):\n",
    "    '''\n",
    "    For a given dataset, pad with zero vectors or truncate to maxlen\n",
    "    '''\n",
    "    new_data = []\n",
    "    \n",
    "    # vector of 0 the length of the word vectors\n",
    "    zero_vector = []\n",
    "    for _ in range(len(data[0][0])):\n",
    "        zero_vector.append(0.0)\n",
    "    \n",
    "    #Iterate through rows, truncate if too big, add zero vectors if too small\n",
    "    for sample in data:\n",
    "        temp = []\n",
    "        if len(sample) > maxlen:\n",
    "            temp = sample[:maxlen]\n",
    "        elif len(sample) < maxlen:\n",
    "            temp = sample\n",
    "            additional_elems = maxlen-len(sample)\n",
    "            for _ in range(additional_elems):\n",
    "                temp.append(zero_vector)\n",
    "        else:\n",
    "            temp = sample\n",
    "        new_data.append(temp)\n",
    "    return new_data\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load test and training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "x_train = pad_trunc(x_train, maxlen)\n",
    "x_test = pad_trunc(x_test, maxlen)\n",
    "\n",
    "x_train = np.reshape(x_train, (len(x_train), maxlen, embedding_dims))\n",
    "y_train = np.array(y_train)\n",
    "x_test = np.reshape(x_test, (len(x_test), maxlen, embedding_dims))\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Recurrent Neural Network Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tomjoshi/anaconda3/envs/nlpiaenv/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_2 (SimpleRNN)     (None, 400, 50)           17550     \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 400, 50)           0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 20000)             0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 20001     \n",
      "=================================================================\n",
      "Total params: 37,551\n",
      "Trainable params: 37,551\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, SimpleRNN\n",
    "num_neurons = 50\n",
    "model = Sequential()\n",
    "\n",
    "model.add(SimpleRNN(\n",
    "    num_neurons, return_sequences=True,\n",
    "    input_shape=(maxlen, embedding_dims)))\n",
    "model.add(Dropout(0.2))\n",
    "#Dense layer expects flat vector of n elements\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile('rmsprop', 'binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Users/tomjoshi/anaconda3/envs/nlpiaenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/2\n",
      "20000/20000 [==============================] - 143s 7ms/step - loss: 0.5785 - accuracy: 0.7096 - val_loss: 0.6205 - val_accuracy: 0.7342\n",
      "Epoch 2/2\n",
      "20000/20000 [==============================] - 163s 8ms/step - loss: 0.4237 - accuracy: 0.8097 - val_loss: 0.4648 - val_accuracy: 0.7948\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fc3d0b17f50>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test,y_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_structure = model.to_json()\n",
    "with open(\"simplernn_model1.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "model.save_weights(\"simplernn_weights1.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making improvements to the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_3 (SimpleRNN)     (None, 400, 100)          40100     \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 400, 100)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 40000)             0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 40001     \n",
      "=================================================================\n",
      "Total params: 80,101\n",
      "Trainable params: 80,101\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 20000 samples, validate on 5000 samples\n",
      "Epoch 1/5\n",
      "20000/20000 [==============================] - 165s 8ms/step - loss: 0.6758 - accuracy: 0.6809 - val_loss: 0.5651 - val_accuracy: 0.7518\n",
      "Epoch 2/5\n",
      "20000/20000 [==============================] - 165s 8ms/step - loss: 0.4458 - accuracy: 0.8062 - val_loss: 0.6874 - val_accuracy: 0.7156\n",
      "Epoch 3/5\n",
      "20000/20000 [==============================] - 146s 7ms/step - loss: 0.3381 - accuracy: 0.8545 - val_loss: 0.5524 - val_accuracy: 0.7762\n",
      "Epoch 4/5\n",
      "20000/20000 [==============================] - 159s 8ms/step - loss: 0.2718 - accuracy: 0.8864 - val_loss: 0.5792 - val_accuracy: 0.7876\n",
      "Epoch 5/5\n",
      "20000/20000 [==============================] - 168s 8ms/step - loss: 0.2210 - accuracy: 0.9082 - val_loss: 0.6169 - val_accuracy: 0.7974\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.callbacks.History at 0x7fc3d1415ad0>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_neurons = 100\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(\n",
    "    num_neurons, \n",
    "    return_sequences=True,\n",
    "    input_shape=(maxlen, embedding_dims)))\n",
    "model.add(Dropout(0.2))\n",
    "#Dense layer expects flat vector of n elements\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1, activation=\"sigmoid\"))\n",
    "model.compile('rmsprop', 'binary_crossentropy', metrics=['accuracy'])\n",
    "model.summary()\n",
    "\n",
    "model.fit(x_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_structure = model.to_json()\n",
    "with open(\"models_and_json/simplernn_model2.json\", \"w\") as json_file:\n",
    "    json_file.write(model_structure)\n",
    "model.save_weights(\"models_and_json/simplernn_weights2.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1 = \"I hate that the dismal weather had me down for so long, when will it break! Ugh, when does happiness return? The sun is blinding and the puffy clouds are too thin. I can't wait for the weekend.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0]], dtype=int32)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.models import model_from_json\n",
    "\n",
    "with open(\"models_and_json/simplernn_model2.json\", \"r\") as json_file:\n",
    "    model_json = json_file.read()\n",
    "model = model_from_json(model_json)\n",
    "model.load_weights('models_and_json/simplernn_weights2.h5')\n",
    "\n",
    "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
    "test_vec_list = pad_trunc(vec_list, maxlen)\n",
    "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
    "model.predict_classes(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
