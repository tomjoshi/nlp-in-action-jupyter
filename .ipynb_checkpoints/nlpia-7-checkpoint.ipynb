{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation\n",
    "from keras.layers import Conv1D, GlobalMaxPooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import os\n",
    "from random import shuffle\n",
    "\n",
    "\n",
    "def preprocess_data(file_path):\n",
    "    pos_path = os.path.join(file_path, \"pos\")\n",
    "    neg_path = os.path.join(file_path, \"neg\")\n",
    "    pos_label = 1\n",
    "    neg_label = 0\n",
    "    dataset = []\n",
    "    \n",
    "    for filename in glob.glob(os.path.join(pos_path, \"*.txt\")):\n",
    "        with open(filename, \"r\") as f:\n",
    "            dataset.append((pos_label, f.read()))\n",
    "\n",
    "    for filename in glob.glob(os.path.join(pos_path, \"*.txt\")):\n",
    "        with open(filename, \"r\") as f:\n",
    "            dataset.append((pos_label, f.read()))\n",
    "    shuffle(dataset)\n",
    "    \n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1,\n",
       " \"Hold Your Man finds Jean Harlow, working class girl from Brooklyn falling for con man Clark Gable and getting in all kinds of trouble. The film starts out as his film, but by the time it's over the emphasis definitely switches to her character.<br /><br />The film opens with Gable pulling a street con game with partner, Garry Owen and the mark yelling for the cops. As he's being chased Gable ducks into Harlow's apartment and being he's such a charming fellow, she shields him.<br /><br />Before long she's involved with him and unfortunately with his rackets. Gable, Harlow, and Owen try pulling a badger game on a drunken Paul Hurst, but then Gable won't go through with it. Of course when Hurst realizes it was a con, he's still sore and gets belligerent and Gable has to punch him out. But then he winds up dead outside Harlow's apartment and that platinum blond hair makes her easy to identify. She goes up on an accomplice to manslaughter.<br /><br />The rest of the film is her's and her adjustment to prison life. Her interaction with the other female prisoners give her some very good scenes. I think some of the material was later used for the MGM classic Caged.<br /><br />Harlow also gets to do the title song and it's done as torch style ballad, very popular back in those days. She talk/sings it in the manner of Sophie Tucker and quite well. <br /><br />Gable is well cast as the con man who develops a conscience, a part he'd play often, most notably in my favorite Gable film, Honky Tonk.<br /><br />Still it's Harlow who gets to shine in this film. I think it's one of the best she did at MGM, her fans should not miss it.\")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = preprocess_data('../Datasets/aclimdb/train')\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizer and tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import TreebankWordTokenizer\n",
    "from gensim.models.keyedvectors import KeyedVectors\n",
    "word_vectors = KeyedVectors.load_word2vec_format('../GoogleNews-vectors-negative300.bin.gz', binary=True, limit=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_vectorize(dataset):\n",
    "    tokenizer = TreebankWordTokenizer()\n",
    "    vectorized_data = []\n",
    "    for sample in dataset:\n",
    "        tokens = tokenizer.tokenize(sample[1])\n",
    "        sample_vecs = []\n",
    "        for token in tokens: \n",
    "            try:\n",
    "                sample_vecs.append(word_vectors[token])\n",
    "            except KeyError:\n",
    "                pass\n",
    "        vectorized_data.append(sample_vecs)\n",
    "    return vectorized_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collect_expected(dataset):\n",
    "    '''\n",
    "    Grab the target values from the dataset we created\n",
    "    '''\n",
    "    expected = []\n",
    "    for sample in dataset:\n",
    "        expected.append(sample[0])\n",
    "    return expected"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_data = tokenize_and_vectorize(dataset)\n",
    "expected = collect_expected(dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "split_point = int(len(vectorized_data)*0.8)\n",
    "x_train = vectorized_data[:split_point]\n",
    "y_train = expected[:split_point]\n",
    "x_test = vectorized_data[split_point:]\n",
    "y_test = expected[split_point:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxlen = 400\n",
    "# How many samples to show the net before backpropagating the error and updating the weights\n",
    "batch_size = 32\n",
    "# length of the token vectors you'll create for passing into the convent\n",
    "embedding_dims = 300\n",
    "filters = 250\n",
    "# embedding_dims * kernel_size \n",
    "kernel_size = 3\n",
    "hidden_dims = 250\n",
    "epochs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Padding and Truncating Token Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_trunc(data, maxlen):\n",
    "    '''\n",
    "    For a given dataset, pad with zero vectors or truncate to maxlen\n",
    "    '''\n",
    "    new_data = []\n",
    "    \n",
    "    # vector of 0 the length of the word vectors\n",
    "    zero_vector = []\n",
    "    for _ in range(len(data[0][0])):\n",
    "        zero_vector.append(0.0)\n",
    "    \n",
    "    #Iterate through rows, truncate if too big, add zero vectors if too small\n",
    "    for sample in data:\n",
    "        temp = []\n",
    "        if len(sample) > maxlen:\n",
    "            temp = sample[:maxlen]\n",
    "        elif len(sample) < maxlen:\n",
    "            temp = sample\n",
    "            additional_elems = maxlen-len(sample)\n",
    "            for _ in range(additional_elems):\n",
    "                temp.append(zero_vector)\n",
    "        else:\n",
    "            temp = sample\n",
    "        new_data.append(temp)\n",
    "    return new_data\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.13769531 -0.06542969  0.00628662  0.08496094 -0.22167969 -0.13964844\n",
      "  0.0267334   0.14160156  0.05126953  0.3359375  -0.36914062 -0.13964844\n",
      " -0.01855469  0.15039062 -0.4140625   0.20410156  0.00369263  0.07763672\n",
      " -0.06835938  0.0612793   0.26171875 -0.0456543   0.25        0.24316406\n",
      "  0.03857422 -0.05737305  0.33984375  0.08007812 -0.00579834 -0.15917969\n",
      "  0.23535156  0.00866699 -0.41601562 -0.22460938  0.14453125  0.08398438\n",
      "  0.14746094  0.16992188 -0.02258301 -0.23925781 -0.30859375 -0.16796875\n",
      "  0.05200195 -0.10546875  0.12109375 -0.33007812 -0.11035156 -0.29492188\n",
      "  0.01647949 -0.05297852 -0.25976562  0.17089844  0.0859375   0.24023438\n",
      "  0.05615234 -0.06689453 -0.16992188  0.18847656 -0.13574219  0.24609375\n",
      " -0.20996094  0.00823975  0.11230469  0.0267334   0.13476562  0.00952148\n",
      " -0.078125    0.31640625  0.09326172  0.35546875 -0.01599121 -0.2265625\n",
      "  0.05859375  0.3515625  -0.16699219 -0.32617188  0.18457031 -0.08447266\n",
      "  0.28320312  0.17480469  0.16601562 -0.34179688  0.30078125 -0.23535156\n",
      " -0.14355469 -0.00127411  0.33398438  0.01153564  0.15917969  0.15429688\n",
      " -0.09521484  0.21875     0.31835938  0.04272461 -0.10595703  0.08886719\n",
      "  0.14746094  0.04003906  0.234375    0.13183594 -0.00619507 -0.4140625\n",
      " -0.16503906 -0.4375     -0.0703125   0.08886719 -0.01409912  0.06933594\n",
      " -0.26171875 -0.06640625 -0.24902344 -0.265625   -0.1640625  -0.51953125\n",
      " -0.01782227  0.35351562 -0.26171875  0.10449219 -0.0703125  -0.36328125\n",
      "  0.0456543  -0.11523438  0.01696777 -0.09472656  0.19335938 -0.05737305\n",
      " -0.09960938  0.00695801  0.03955078  0.06884766  0.23828125 -0.08007812\n",
      "  0.11572266 -0.1875      0.2890625  -0.17773438  0.13574219 -0.06738281\n",
      " -0.2421875   0.20214844 -0.01287842 -0.0859375  -0.14453125 -0.27539062\n",
      " -0.18066406 -0.00424194 -0.15429688 -0.22070312  0.6953125  -0.10449219\n",
      " -0.03417969 -0.02575684  0.13574219 -0.14550781  0.04003906  0.06884766\n",
      " -0.23730469 -0.06103516 -0.24023438 -0.15332031 -0.31054688 -0.18847656\n",
      "  0.36523438  0.01489258  0.22363281  0.36132812  0.06738281 -0.20214844\n",
      "  0.10302734  0.27539062 -0.11621094 -0.078125    0.04760742 -0.09472656\n",
      " -0.20800781 -0.078125    0.03710938  0.05883789  0.11523438 -0.04174805\n",
      " -0.51171875 -0.05761719  0.08886719  0.10595703 -0.14746094 -0.14453125\n",
      "  0.23144531 -0.41210938 -0.02905273  0.2421875  -0.08886719  0.08935547\n",
      "  0.06835938  0.01782227 -0.02392578 -0.00463867 -0.27539062  0.10986328\n",
      "  0.15234375 -0.07958984 -0.03808594  0.00610352 -0.32617188 -0.01660156\n",
      " -0.09130859 -0.09814453 -0.4375     -0.04541016  0.01495361  0.14355469\n",
      " -0.02844238  0.20507812 -0.01538086  0.29492188 -0.17773438 -0.09375\n",
      "  0.02685547 -0.04345703  0.16699219 -0.578125   -0.20996094  0.41015625\n",
      " -0.50390625 -0.20898438  0.17285156 -0.21484375 -0.19042969 -0.38671875\n",
      "  0.3125      0.29882812  0.09228516 -0.11376953  0.15234375  0.12792969\n",
      "  0.03442383 -0.25976562 -0.22460938 -0.31640625  0.22070312  0.24707031\n",
      "  0.24121094  0.4296875  -0.20996094 -0.01782227  0.43945312  0.13476562\n",
      "  0.00637817  0.13085938  0.04052734 -0.00878906 -0.5078125   0.3515625\n",
      "  0.11230469 -0.07958984  0.16796875 -0.18847656 -0.21386719 -0.20996094\n",
      "  0.15722656 -0.10107422  0.140625    0.3515625   0.37109375 -0.07177734\n",
      " -0.09472656  0.28320312 -0.45507812 -0.05053711  0.06982422  0.04516602\n",
      "  0.25        0.22265625  0.36328125 -0.04833984  0.09619141  0.18945312\n",
      " -0.18945312 -0.14257812  0.05322266  0.08447266  0.2890625   0.07324219\n",
      "  0.08837891  0.13378906 -0.1171875   0.07421875 -0.16015625  0.21386719\n",
      "  0.0291748   0.13867188  0.05102539  0.1171875  -0.33789062  0.06445312\n",
      "  0.26953125 -0.36914062  0.1875     -0.09765625 -0.11914062  0.01135254]\n",
      "[-1.62109375e-01  8.69140625e-02  2.01171875e-01  4.24804688e-02\n",
      " -1.03027344e-01  1.67968750e-01  1.22070312e-01  6.00585938e-02\n",
      "  1.98242188e-01 -3.17382812e-02 -1.68945312e-01 -2.91015625e-01\n",
      " -1.45507812e-01  5.17578125e-02 -2.16796875e-01  9.42382812e-02\n",
      "  2.04101562e-01  4.15039062e-02  1.15722656e-01 -1.98242188e-01\n",
      " -2.95410156e-02 -4.02832031e-02  3.35937500e-01 -2.40234375e-01\n",
      "  1.31835938e-01 -2.83203125e-01 -1.30859375e-01  4.73632812e-02\n",
      "  2.11181641e-02  2.33398438e-01  3.54003906e-02  6.78710938e-02\n",
      " -1.72851562e-01 -3.22265625e-02 -1.36718750e-01  1.26953125e-01\n",
      "  3.88183594e-02  1.99218750e-01  7.42187500e-02  1.78710938e-01\n",
      "  2.23388672e-02 -1.39648438e-01  2.08984375e-01 -2.79296875e-01\n",
      "  1.03759766e-02  1.46484375e-01  3.46679688e-02 -2.77343750e-01\n",
      "  1.84570312e-01  8.69140625e-02 -1.02539062e-01  2.71484375e-01\n",
      "  8.15429688e-02 -2.16796875e-01  1.86523438e-01  5.61523438e-02\n",
      "  1.13281250e-01 -2.15820312e-01  2.73437500e-01  6.78710938e-02\n",
      " -2.50244141e-02 -7.22656250e-02 -3.45703125e-01 -1.20117188e-01\n",
      " -1.57226562e-01 -1.08886719e-01  1.40625000e-01  1.25976562e-01\n",
      "  7.51953125e-02 -2.09960938e-01  1.70898438e-01  2.24609375e-01\n",
      "  2.23388672e-02  1.82617188e-01 -3.47656250e-01 -2.40234375e-01\n",
      "  4.83398438e-02  3.78417969e-02  2.87109375e-01  1.67968750e-01\n",
      " -5.98144531e-02  1.49414062e-01 -1.64062500e-01  1.50390625e-01\n",
      " -1.98242188e-01 -2.50000000e-01 -1.52343750e-01  4.23828125e-01\n",
      "  3.64303589e-04  1.59179688e-01  1.29882812e-01  1.90429688e-01\n",
      " -1.26953125e-01 -2.16796875e-01 -1.03149414e-02 -2.96020508e-03\n",
      "  2.87109375e-01  7.56835938e-02 -1.48437500e-01  3.08593750e-01\n",
      "  4.73022461e-03 -1.68945312e-01  1.83593750e-01  1.70898438e-01\n",
      " -1.36108398e-02 -1.99218750e-01 -1.88476562e-01 -6.78710938e-02\n",
      " -6.29882812e-02 -1.15356445e-02  1.22070312e-01 -1.57226562e-01\n",
      "  1.26953125e-01  1.80664062e-01  1.88476562e-01  4.54101562e-02\n",
      " -1.69921875e-01  2.79296875e-01 -9.86328125e-02 -7.53784180e-03\n",
      " -8.20312500e-02 -6.15234375e-02  9.76562500e-02  2.70996094e-02\n",
      "  2.03125000e-01 -9.03320312e-03  6.88476562e-02 -5.71289062e-02\n",
      " -2.55126953e-02 -2.03125000e-01 -8.10546875e-02 -1.30859375e-01\n",
      " -5.07812500e-02  6.29882812e-02  2.59765625e-01 -3.76953125e-01\n",
      " -3.08593750e-01 -2.08984375e-01 -1.69921875e-01  4.14062500e-01\n",
      "  1.18652344e-01 -1.26953125e-01 -2.59765625e-01  6.07910156e-02\n",
      "  3.80859375e-01 -1.90429688e-01  4.76074219e-02 -3.96484375e-01\n",
      " -9.57031250e-02  1.20605469e-01  1.86767578e-02 -4.10156250e-02\n",
      "  5.83496094e-02  1.98974609e-02  1.15234375e-01 -3.39843750e-01\n",
      "  1.31835938e-01  1.79687500e-01 -2.55859375e-01  1.04003906e-01\n",
      "  1.69677734e-02  2.70996094e-02 -4.95910645e-04  1.68945312e-01\n",
      "  1.67968750e-01  3.44238281e-02 -1.13769531e-01  1.24023438e-01\n",
      " -3.14453125e-01  6.00585938e-02 -2.79296875e-01  3.80859375e-01\n",
      "  4.83398438e-02  4.78515625e-02 -5.27954102e-03  1.73828125e-01\n",
      "  7.27539062e-02 -1.18652344e-01 -9.66796875e-02 -1.80664062e-01\n",
      " -1.04980469e-01 -1.97265625e-01 -5.56640625e-02  3.35937500e-01\n",
      "  9.03320312e-02  1.28906250e-01 -1.16210938e-01  2.18505859e-02\n",
      "  4.85839844e-02  3.27148438e-02  2.62451172e-02 -7.17163086e-03\n",
      " -1.37695312e-01  5.15136719e-02  8.54492188e-02 -3.41796875e-01\n",
      " -7.71484375e-02  4.44335938e-02 -6.64062500e-02 -2.75390625e-01\n",
      " -5.49316406e-02 -1.71875000e-01  1.78710938e-01 -1.32812500e-01\n",
      " -8.54492188e-02 -3.39355469e-02 -1.10351562e-01 -1.11328125e-01\n",
      "  2.41210938e-01 -2.23632812e-01 -7.17773438e-02  2.10937500e-01\n",
      " -2.42187500e-01 -6.89697266e-03 -9.52148438e-02  1.63085938e-01\n",
      "  1.15966797e-02  1.40625000e-01 -1.27563477e-02  1.64062500e-01\n",
      "  1.07421875e-01 -5.98144531e-02 -4.51660156e-02  9.17968750e-02\n",
      "  6.83593750e-02 -2.46582031e-02  2.43164062e-01 -3.78417969e-02\n",
      " -9.94873047e-03 -1.55273438e-01 -5.54199219e-02 -1.07421875e-01\n",
      " -8.15429688e-02  1.03515625e-01  4.71191406e-02  5.22460938e-02\n",
      " -4.90722656e-02  2.13867188e-01 -1.01928711e-02  1.66992188e-01\n",
      " -6.68945312e-02 -9.88769531e-03  6.39648438e-02 -4.95605469e-02\n",
      "  2.24304199e-03 -5.46875000e-02  1.16210938e-01  2.81982422e-02\n",
      "  1.27929688e-01 -1.28906250e-01  4.31640625e-01  1.79443359e-02\n",
      " -1.45263672e-02  1.81640625e-01  3.43750000e-01 -8.98437500e-02\n",
      "  2.32421875e-01  4.29687500e-02  7.51953125e-02 -1.37329102e-02\n",
      " -2.02148438e-01  8.44726562e-02 -3.45703125e-01  1.38671875e-01\n",
      " -6.05468750e-02  2.08984375e-01 -8.88671875e-02  1.82617188e-01\n",
      " -5.90820312e-02  2.51464844e-02  9.61914062e-02  2.23632812e-01\n",
      "  1.95312500e-01  1.83593750e-01 -3.20312500e-01 -6.25000000e-02\n",
      "  1.55273438e-01 -2.08984375e-01 -8.25195312e-02  2.23388672e-02\n",
      "  5.78613281e-02  6.29882812e-02  3.66210938e-02  3.65234375e-01\n",
      " -2.51770020e-03  4.24804688e-02 -8.10546875e-02 -2.77343750e-01\n",
      "  1.36718750e-01  1.60156250e-01 -1.06445312e-01  2.25585938e-01\n",
      " -9.17968750e-02  8.30078125e-02  9.91210938e-02 -1.59179688e-01\n",
      "  5.85937500e-02 -8.39843750e-02  4.49218750e-02 -1.79687500e-01]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'embeddings_dims' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-5c935421a32a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mX_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mX_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings_dims\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'embeddings_dims' is not defined"
     ]
    }
   ],
   "source": [
    "X_train = pad_trunc(X_train, maxlen)\n",
    "X_test = pad_trunc(X_test, maxlen)\n",
    "\n",
    "X_train = np.reshape(X_train, (len(X_train), maxlen, embedding_dims))\n",
    "y_train = np.array(y_train)\n",
    "X_test = np.reshape(X_test, (len(X_test, maxlen, embeddings_dims)))\n",
    "y_test = np.array(y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## Construct 1D CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Build model...\n",
      "WARNING:tensorflow:From /Users/tomjoshi/anaconda3/envs/nlpiaenv/lib/python3.7/site-packages/tensorflow_core/python/ops/nn_impl.py:183: where (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
     ]
    }
   ],
   "source": [
    "print('Build model...')\n",
    "model = Sequential()\n",
    "\n",
    "# learns word group filters\n",
    "model.add(Conv1D(\n",
    "    filters, \n",
    "    kernel_size,\n",
    "    padding='valid',\n",
    "    activation='relu',\n",
    "    strides=1,\n",
    "    input_shape=(maxlen, embedding_dims)))\n",
    "\n",
    "model.add(GlobalMaxPooling1D())\n",
    "model.add(Dense(hidden_dims))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Activation('relu'))\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 5000 arrays: [array([[-0.16210938,  0.08691406,  0.20117188, ..., -0.08398438,\n         0.04492188, -0.1796875 ],\n       [ 0.20410156,  0.01318359,  0.07568359, ..., -0.21191406,\n        -0.1328125 ,  0.10839844],...",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-28d90c1782f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m           \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m           \u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m           validation_data=(X_test, y_test))\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/nlpiaenv/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m   1173\u001b[0m                 \u001b[0mval_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1174\u001b[0m                 \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1175\u001b[0;31m                 batch_size=batch_size)\n\u001b[0m\u001b[1;32m   1176\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_uses_dynamic_learning_phase\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1177\u001b[0m                 \u001b[0mval_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_x\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_y\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mval_sample_weights\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlpiaenv/lib/python3.7/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[0;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[1;32m    577\u001b[0m             \u001b[0mfeed_input_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    578\u001b[0m             \u001b[0mcheck_batch_axis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# Don't enforce the batch size.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 579\u001b[0;31m             exception_prefix='input')\n\u001b[0m\u001b[1;32m    580\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/nlpiaenv/lib/python3.7/site-packages/keras/engine/training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[0;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[1;32m    107\u001b[0m                 \u001b[0;34m'Expected to see '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m' array(s), '\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    108\u001b[0m                 \u001b[0;34m'but instead got the following list of '\u001b[0m \u001b[0;34m+\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 109\u001b[0;31m                 str(len(data)) + ' arrays: ' + str(data)[:200] + '...')\n\u001b[0m\u001b[1;32m    110\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnames\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m             raise ValueError(\n",
      "\u001b[0;31mValueError\u001b[0m: Error when checking model input: the list of Numpy arrays that you are passing to your model is not the size the model expected. Expected to see 1 array(s), but instead got the following list of 5000 arrays: [array([[-0.16210938,  0.08691406,  0.20117188, ..., -0.08398438,\n         0.04492188, -0.1796875 ],\n       [ 0.20410156,  0.01318359,  0.07568359, ..., -0.21191406,\n        -0.1328125 ,  0.10839844],..."
     ]
    }
   ],
   "source": [
    "model.fit(X_train, y_train,\n",
    "          batch_size=batch_size,\n",
    "          epochs=epochs,\n",
    "          validation_data=(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_structure = model.to_json()\n",
    "with open(\"cnn_model.json\", \"w\") as json_file: \n",
    "    json_file.write(model_structure)\n",
    "model.save_weights(\"cnn_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import model_from_json\n",
    "with open(\"cnn_model.json\", \"r\") as json_file:\n",
    "    json_string = json_file.read()\n",
    "\n",
    "model = model_from_json(json_string)\n",
    "model.load_weights(\"cnn_weights.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_1 = \"I hate that the dismal weather had me down for so long, when will it break! Ugh, when does happiness return? The sun is blinding and the puffy clouds are too thin. I can't wait for the weekend.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.07910156 -0.0050354   0.11181641  0.21289062  0.13085938 -0.01470947\n",
      " -0.03540039 -0.07763672  0.04077148  0.11474609  0.00147247 -0.29101562\n",
      "  0.00457764 -0.20019531 -0.19238281  0.08007812  0.10107422  0.04858398\n",
      "  0.15722656 -0.09521484 -0.05004883  0.25        0.33007812 -0.09716797\n",
      " -0.05566406 -0.0071106  -0.16796875 -0.13574219  0.05102539 -0.00598145\n",
      "  0.10791016  0.16503906 -0.03955078 -0.03955078  0.04321289  0.12060547\n",
      "  0.13476562  0.09375     0.00909424  0.1640625   0.21289062 -0.05322266\n",
      "  0.33398438  0.01586914  0.10449219  0.24121094 -0.0189209  -0.04199219\n",
      "  0.05834961  0.03271484  0.09863281  0.18945312  0.04125977  0.01501465\n",
      " -0.05883789  0.10253906  0.01538086  0.03198242  0.02722168 -0.13769531\n",
      "  0.12695312  0.06396484 -0.13574219 -0.012146    0.07617188 -0.02319336\n",
      " -0.21191406  0.20996094 -0.01953125  0.02038574  0.16113281 -0.00897217\n",
      "  0.04663086  0.03881836 -0.4609375  -0.1796875   0.12792969 -0.00564575\n",
      "  0.24121094  0.21777344 -0.02600098 -0.1171875   0.19140625 -0.04052734\n",
      " -0.18261719 -0.09619141 -0.05053711  0.27734375  0.00086975  0.06298828\n",
      " -0.13085938  0.1328125  -0.06640625  0.03491211 -0.26953125  0.0267334\n",
      "  0.11816406  0.1328125  -0.02490234 -0.17480469 -0.234375   -0.0703125\n",
      "  0.09179688  0.03198242 -0.10791016  0.13671875 -0.11523438  0.00732422\n",
      "  0.04223633 -0.25585938 -0.27734375 -0.10644531  0.02612305  0.09082031\n",
      "  0.14355469 -0.19335938 -0.07421875 -0.20898438  0.21289062 -0.17382812\n",
      " -0.04980469 -0.10839844 -0.02038574  0.22949219 -0.09570312 -0.06103516\n",
      " -0.10693359  0.03369141  0.01879883  0.05029297 -0.36132812 -0.24511719\n",
      " -0.34765625 -0.04394531 -0.18261719 -0.14453125 -0.06152344  0.21386719\n",
      "  0.09667969  0.1796875  -0.02160645  0.03100586  0.0703125  -0.07275391\n",
      "  0.07226562  0.10644531 -0.25       -0.20800781 -0.0378418  -0.01446533\n",
      "  0.171875    0.17578125 -0.33789062  0.05517578 -0.01965332  0.13378906\n",
      " -0.01647949 -0.04541016 -0.16796875  0.08544922  0.09960938 -0.02001953\n",
      " -0.06347656  0.32421875 -0.02404785 -0.22949219  0.09960938  0.08886719\n",
      "  0.13769531 -0.05517578 -0.2109375   0.1875     -0.03833008 -0.16796875\n",
      " -0.11132812  0.05249023  0.1875     -0.15136719  0.07275391 -0.01953125\n",
      " -0.06933594  0.16894531  0.1171875   0.10351562  0.02905273  0.0546875\n",
      " -0.2578125   0.1171875   0.0703125  -0.04711914 -0.03710938  0.16503906\n",
      " -0.01159668 -0.04638672  0.11621094  0.09521484 -0.09130859  0.01916504\n",
      " -0.08886719 -0.20507812 -0.13183594  0.12060547  0.04980469  0.09863281\n",
      "  0.02819824  0.046875   -0.18554688 -0.06347656  0.07519531  0.11767578\n",
      "  0.08154297  0.0144043  -0.11132812  0.00939941 -0.23828125 -0.1796875\n",
      "  0.265625    0.04174805 -0.0859375  -0.06298828 -0.1953125   0.03588867\n",
      " -0.03295898  0.04174805  0.23339844 -0.3515625   0.16503906 -0.0625\n",
      " -0.15136719 -0.10009766  0.03979492 -0.01342773  0.0123291  -0.00939941\n",
      "  0.12109375  0.06738281 -0.18457031  0.15917969  0.12109375 -0.1171875\n",
      "  0.00302124  0.11425781  0.11767578 -0.07666016 -0.16503906 -0.09570312\n",
      " -0.09277344  0.12792969 -0.09375     0.04223633  0.21679688 -0.0859375\n",
      " -0.05883789  0.16015625  0.03710938  0.01708984  0.00326538 -0.13183594\n",
      " -0.17480469 -0.27539062 -0.03417969 -0.10546875  0.07226562 -0.05541992\n",
      "  0.09228516  0.18164062  0.04003906  0.07275391 -0.14355469  0.13378906\n",
      " -0.05371094  0.15234375  0.265625    0.24609375 -0.05419922 -0.07177734\n",
      " -0.08496094 -0.25585938 -0.08300781 -0.03271484  0.08203125  0.00077057\n",
      "  0.0480957   0.17089844  0.13964844  0.03588867 -0.06542969 -0.2734375\n",
      " -0.22753906  0.09570312 -0.12597656  0.15332031 -0.30664062 -0.07861328\n",
      " -0.08642578 -0.11474609 -0.02929688 -0.0067749   0.04272461 -0.10351562]\n",
      "WARNING:tensorflow:From /Users/tomjoshi/anaconda3/envs/nlpiaenv/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[0.5245353]], dtype=float32)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_list = tokenize_and_vectorize([(1, sample_1)])\n",
    "test_vec_list = pad_trunc(vec_list, maxlen)\n",
    "test_vec = np.reshape(test_vec_list, (len(test_vec_list), maxlen, embedding_dims))\n",
    "model.predict(test_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
